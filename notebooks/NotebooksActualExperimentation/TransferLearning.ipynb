{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code of the paper related to measurements and how they relate to the Shapley Value (SV) computation. This code is mainly divided in two parts:\n",
    "\n",
    "- Computation of the measures in the different datasets.\n",
    "- Computation of the correlation between the measures and the SV computation in Federated Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Computation (1st Part)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..' + os.sep + '..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import xgboost as xgb\n",
    "import prince\n",
    "import scipy\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_parameters.model_builder.Model import XGBoostModel\n",
    "from experiment_parameters.TrainerFactory import dataset_model_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "\n",
    "These are mainly functions to read data or results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_ROUTE_MAIN_DIR = \"..\" + os.sep + \"..\" + os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_route(dataset_name, type_of_partition, additional_parameter):\n",
    "    if type_of_partition == \"manual\":\n",
    "        path_to_train_datasets = CONST_ROUTE_MAIN_DIR + \"data\" + os.sep + \"partitioned_training_data\" + os.sep + type_of_partition + os.sep + additional_parameter\n",
    "    else:\n",
    "        path_to_train_datasets = CONST_ROUTE_MAIN_DIR + \"data\" + os.sep + \"partitioned_training_data\" + os.sep + type_of_partition + os.sep + \"dataset_\" + dataset_name + os.sep + \"alpha_\" + additional_parameter\n",
    "\n",
    "    return path_to_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_route(dataset_name, type_of_partition, additional_parameter):\n",
    "    # path_to_result_dataframes = CONST_ROUTE_MAIN_DIR + \"results\" + os.sep + \"FedAvg\" + os.sep + dataset_name + os.sep + type_of_partition + os.sep + additional_parameter + os.sep + \"mlp\"\n",
    "    if type_of_partition == \"dirichlet\":\n",
    "        path_to_result_dataframes = CONST_ROUTE_MAIN_DIR + \"results\" + os.sep + \"dataframes\" + os.sep + \"FedAvg\" + os.sep + dataset_name + os.sep + type_of_partition + os.sep + \"alpha_\" + additional_parameter + os.sep + \"mlp\"\n",
    "    else:\n",
    "        path_to_result_dataframes = CONST_ROUTE_MAIN_DIR + \"results\" + os.sep + \"dataframes\" + os.sep + \"FedAvg\" + os.sep + dataset_name + os.sep + type_of_partition + os.sep + additional_parameter + os.sep + \"mlp\"\n",
    "    return path_to_result_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances_from_route(dataset_name, type_of_partition, additional_parameter):\n",
    "    path_to_result_dataframes = CONST_ROUTE_MAIN_DIR + \"results\" + os.sep + \"distances_values\" + os.sep + dataset_name + os.sep + type_of_partition + os.sep + additional_parameter\n",
    "    return path_to_result_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def number_of_clients_and_all_combinations(path_to_train_datasets):\n",
    "    num_clients = int(len(os.listdir(path_to_train_datasets)) / 4)\n",
    "    client_numbers_original_order = list(range(num_clients))\n",
    "    client_numbers_reverse_order = list(range(num_clients - 1, -1, -1))\n",
    "\n",
    "    all_combinations = list(combinations_with_replacement(client_numbers_original_order, 2)) + list(combinations_with_replacement(client_numbers_reverse_order, 2))\n",
    "    all_combinations = sorted(list(set(all_combinations)))\n",
    "    print(all_combinations)\n",
    "    return num_clients, all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_types(dataframe):\n",
    "    for column in dataframe.select_dtypes(\"int\"):\n",
    "        dataframe[column] = dataframe[column].astype(\"int16\")\n",
    "\n",
    "    for column in dataframe.select_dtypes(\"float\"):\n",
    "        dataframe[column] = dataframe[column].astype(\"float32\")\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring functions\n",
    "\n",
    "The following code contains the functions for distribution distances, similarity and volume computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.Distances import compute_coupling, compute_CE\n",
    "from experiment_parameters.TrainerFactory import dataset_model_dictionary\n",
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for the Wasserstein Distance. We use the geomloss library to execute the code.\n",
    "\n",
    "Paper of reference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geomloss import SamplesLoss\n",
    "\n",
    "def wasserstein_distance(src_x, tar_x):\n",
    "    # Define a Sinkhorn (~Wasserstein) loss between sampled measures\n",
    "    loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=0.01, scaling=0.8) # Although the euclidean distance usually square's root the results, it is not done here. No clue.\n",
    "\n",
    "    L = loss(torch.tensor(src_x.values).type(dtype), torch.tensor(tar_x.values).type(dtype))  # By default, use constant weights = 1/number of samples\n",
    "    if use_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    return 2 * L.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for the Maximum Mean Discrepancy (MMD), using the Gaussian Kernel. We use the geomloss library to execute the code.\n",
    "\n",
    "Paper of reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mmd_distance(src_x, tar_x):\n",
    "    # Define a Gaussian MMD loss between sampled measures\n",
    "    loss = SamplesLoss(loss=\"gaussian\", blur=0.05) # Although the euclidean distance usually square's root the results, it is not done here. No clue.\n",
    "\n",
    "    L = loss(torch.tensor(src_x.values).type(dtype), torch.tensor(tar_x.values).type(dtype))  # By default, use constant weights = 1/number of samples\n",
    "    if use_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    return L.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for negative conditional entropy.\n",
    "\n",
    "Paper of reference: Negative Conditional Entropy in `Transferability and Hardness of Supervised Classification Tasks (ICCV 2019) [Paper en Arxiv](https://arxiv.org/pdf/1908.08142v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_conditional_entropy(source_labels: np.ndarray, target_labels: np.ndarray):\n",
    "    r\"\"\"\n",
    "    Negative Conditional Entropy in `Transferability and Hardness of Supervised\n",
    "    Classification Tasks (ICCV 2019) <https://arxiv.org/pdf/1908.08142v1.pdf>`_.\n",
    "\n",
    "    The NCE :math:`\\mathcal{H}` can be described as:\n",
    "\n",
    "    .. math::\n",
    "        \\mathcal{H}=-\\sum_{y \\in \\mathcal{C}_t} \\sum_{z \\in \\mathcal{C}_s} \\hat{P}(y, z) \\log \\frac{\\hat{P}(y, z)}{\\hat{P}(z)}\n",
    "\n",
    "    where :math:`\\hat{P}(z)` is the empirical distribution and :math:`\\hat{P}\\left(y \\mid z\\right)` is the empirical\n",
    "    conditional distribution estimated by source and target label.\n",
    "\n",
    "    Args:\n",
    "        source_labels (np.ndarray): predicted source labels.\n",
    "        target_labels (np.ndarray): groud-truth target labels.\n",
    "\n",
    "    Shape:\n",
    "        - source_labels: (N, ) elements in [0, :math:`C_s`), with source class number :math:`C_s`.\n",
    "        - target_labels: (N, ) elements in [0, :math:`C_t`), with target class number :math:`C_t`.\n",
    "    \"\"\"\n",
    "    C_t = int(np.max(target_labels) + 1)\n",
    "    C_s = int(np.max(source_labels) + 1)\n",
    "    N = len(source_labels)\n",
    "\n",
    "    joint = np.zeros((C_t, C_s), dtype=float)  # placeholder for the joint distribution, shape [C_t, C_s]\n",
    "    for s, t in zip(source_labels, target_labels):\n",
    "        s = int(s)\n",
    "        t = int(t)\n",
    "        joint[t, s] += 1.0 / N\n",
    "    p_z = joint.sum(axis=0, keepdims=True)\n",
    "\n",
    "    p_target_given_source = (joint / p_z).T  # P(y | z), shape [C_s, C_t]\n",
    "    mask = p_z.reshape(-1) != 0  # valid Z, shape [C_s]\n",
    "    p_target_given_source = p_target_given_source[mask] + 1e-20  # remove NaN where p(z) = 0, add 1e-20 to avoid log (0)\n",
    "    entropy_y_given_z = np.sum(- p_target_given_source * np.log(p_target_given_source), axis=1, keepdims=True)\n",
    "    conditional_entropy = np.sum(entropy_y_given_z * p_z.reshape((-1, 1))[mask])\n",
    "\n",
    "    return -conditional_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of functions for the X->Y shift measure.\n",
    "\n",
    "Paper of reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from metrics.Evaluator import evaluator\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    ground_truth_np = np.argmax(y_test, axis=1)\n",
    "    return accuracy_score(ground_truth_np, y_pred)\n",
    "\n",
    "def degradation_decomp(source_X, source_y, other_X_raw, other_y_raw, best_method, column_names, data_sum=20000, K=8, domain_classifier=None, draw_calibration=False, save_calibration_png='calibration.png'):\n",
    "    perm1 = np.random.permutation(other_X_raw.shape[0])\n",
    "    other_X = other_X_raw[perm1[:data_sum],:]\n",
    "    other_y = other_y_raw[perm1[:data_sum]]\n",
    "\n",
    "    piA = np.zeros(source_X.shape[0])\n",
    "    piB = np.zeros(other_X.shape[0])\n",
    "    permA = np.random.permutation(piA.shape[0])\n",
    "    permB = np.random.permutation(piB.shape[0])\n",
    "\n",
    "    kf = KFold(n_splits=K, shuffle=False)\n",
    "    A_train_index_list = []\n",
    "    A_test_index_list = []\n",
    "    B_train_index_list = []\n",
    "    B_test_index_list = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(source_X)):\n",
    "        A_train_index_list.append(train_index)\n",
    "        A_test_index_list.append(test_index)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(other_X)):\n",
    "        B_train_index_list.append(train_index)\n",
    "        B_test_index_list.append(test_index)\n",
    "\n",
    "    for i in range(K):\n",
    "        trainX = np.concatenate([source_X[permA[A_train_index_list[i]]],other_X[permB[B_train_index_list[i]]]], axis=0)\n",
    "        trainT = np.zeros(trainX.shape[0])\n",
    "        trainT[len(A_train_index_list[i]):] = 1.0\n",
    "\n",
    "        if domain_classifier is None:\n",
    "            model = XGBClassifier(random_state=0).fit(trainX, trainT)\n",
    "        else:\n",
    "            model = domain_classifier.fit(trainX, trainT)\n",
    "\n",
    "        piA[permA[A_test_index_list[i]]] = model.predict_proba(source_X[permA[A_test_index_list[i]]])[:,1]\n",
    "        piB[permB[B_test_index_list[i]]] = model.predict_proba(other_X[permB[B_test_index_list[i]]])[:,1]\n",
    "\n",
    "    # if draw_calibration:\n",
    "    #     plot_calibration(piA, piB, save_dir=save_calibration_png)\n",
    "\n",
    "    alpha = (other_X.shape[0])/ (source_X.shape[0]+other_X.shape[0])\n",
    "    wA = piA / ((1-alpha)*piA + alpha * (1-piA))\n",
    "    wB = (1-piB) / ((1-alpha)*piB + alpha * (1-piB))\n",
    "    # Changing to support the model type of XGBoost.\n",
    "    # accuracyA = best_method.score(source_X, source_y)\n",
    "    # accuracyB = best_method.score(other_X, other_y)\n",
    "    pd_source_X = pd.DataFrame(source_X, columns=column_names)\n",
    "    pd_other_X = pd.DataFrame(other_X, columns=column_names)\n",
    "    accuracyA = accuracy(source_y, best_method.predict_proba(pd_source_X))\n",
    "    accuracyB = accuracy(other_y, best_method.predict_proba(pd_other_X))\n",
    "    wA = wA / np.sum(wA)\n",
    "    wB = wB / np.sum(wB)\n",
    "    # predA = (best_method.predict(source_X) == source_y)\n",
    "    # predB = (best_method.predict(other_X) == other_y)\n",
    "    predA = (np.argmax(best_method.predict_proba(pd_source_X), axis=1) == np.argmax(source_y, axis=1))\n",
    "    predB = (np.argmax(best_method.predict_proba(pd_other_X), axis=1) == np.argmax(other_y, axis=1))\n",
    "    sx_A = np.dot(wA, predA)\n",
    "    sx_B = np.dot(wB, predB)\n",
    "    return accuracyA, accuracyB, sx_A, sx_B\n",
    "\n",
    "def y_shift(src_x, src_y, tar_x, tar_y, tree_model, column_names):\n",
    "    p2p, q2q, p2s, s2q = degradation_decomp(src_x, src_y, tar_x, tar_y, tree_model, column_names, data_sum=20000, K=8, draw_calibration=False, save_calibration_png='calibration.png')\n",
    "    # print(f\"Total Performance Degradation is {p2p-q2q}\")\n",
    "    # print(f\"Proportion of Y|X-shift is {(p2s-s2q)/(p2p-q2q)}\")\n",
    "    perf_degradation = p2p-q2q\n",
    "    proportion_yshift = (p2s-s2q)/(p2p-q2q)\n",
    "    return perf_degradation, proportion_yshift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of functions for the Similarity and diversity measure.\n",
    "\n",
    "Paper of reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_agnostic_data_valuation(src_x, tar_x):\n",
    "    cov_mat_src = src_x.cov()\n",
    "    src_eig_vals, src_eig_vecs = np.linalg.eig((cov_mat_src.T @ cov_mat_src) * (1 / len(src_x)))\n",
    "    cov_mat_tar = ((tar_x.cov().T @ tar_x.cov()) * (1 / len(tar_x)))\n",
    "    tar_eig_vals = [np.sqrt(np.sum(np.square(cov_mat_tar.dot(eigen_vec)))) for eigen_vec in src_eig_vecs]\n",
    "    src_eig_vals = np.array(src_eig_vals)\n",
    "    tar_eig_vals = np.array(tar_eig_vals)\n",
    "    diversity, relevance = 1, 1\n",
    "    for src_eig, tar_eig in zip(src_eig_vals, tar_eig_vals):\n",
    "        diversity *= np.power((abs(src_eig - tar_eig) / max(src_eig, tar_eig)), 1 / len(src_eig_vals))\n",
    "        relevance *= np.power((min(src_eig, tar_eig) / max(src_eig, tar_eig)), 1 / len(src_eig_vals))\n",
    "    return relevance, diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of functions for the Robust Volume measure.\n",
    "\n",
    "Paper of reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import stack, cat, zeros_like, pinverse\n",
    "\n",
    "\n",
    "# def compute_volumes(X, d=1):\n",
    "def compute_volumes(datasets, d=1):\n",
    "    d = datasets[0].shape[1]\n",
    "    for i in range(len(datasets)):\n",
    "        datasets[i] = datasets[i].reshape(-1 ,d)\n",
    "\n",
    "    X = np.concatenate(datasets, axis=0).reshape(-1, d)\n",
    "    volumes = np.zeros(len(datasets))\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        volumes[i] = np.sqrt(np.linalg.det( dataset.T @ dataset ) + 1e-8)\n",
    "\n",
    "    volume_all = np.sqrt(np.linalg.det(X.T @ X) + 1e-8).round(3)\n",
    "    return volumes, volume_all\n",
    "    # return volume_all\n",
    "\n",
    "\n",
    "def compute_X_tilde_and_counts(X, omega):\n",
    "    \"\"\"\n",
    "    Compresses the original feature matrix X to  X_tilde with the specified omega.\n",
    "\n",
    "    Returns:\n",
    "       X_tilde: compressed np.ndarray\n",
    "       cubes: a dictionary of cubes with the respective counts in each dcube\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "\n",
    "    # assert 0 < omega <= 1, \"omega must be within range [0,1].\"\n",
    "\n",
    "    m = ceil(1.0 / omega) # number of intervals for each dimension\n",
    "\n",
    "    cubes = Counter() # a dictionary to store the freqs\n",
    "    # key: (1,1,..)  a d-dimensional tuple, each entry between [0, m-1]\n",
    "    # value: counts\n",
    "\n",
    "    Omega = defaultdict(list)\n",
    "    # Omega = {}\n",
    "\n",
    "    min_ds = torch.min(X, axis=0).values\n",
    "\n",
    "    # a dictionary to store cubes of not full size\n",
    "    for x in X:\n",
    "        cube = []\n",
    "        for d, xd in enumerate(x - min_ds):\n",
    "            d_index = floor(xd / omega)\n",
    "            cube.append(d_index)\n",
    "\n",
    "        cube_key = tuple(cube)\n",
    "        cubes[cube_key] += 1\n",
    "\n",
    "        Omega[cube_key].append(x)\n",
    "\n",
    "        '''\n",
    "        if cube_key in Omega:\n",
    "\n",
    "            # Implementing mean() to compute the average of all rows which fall in the cube\n",
    "\n",
    "            Omega[cube_key] = Omega[cube_key] * (1 - 1.0 / cubes[cube_key]) + 1.0 / cubes[cube_key] * x\n",
    "            # Omega[cube_key].append(x)\n",
    "        else:\n",
    "             Omega[cube_key] = x\n",
    "        '''\n",
    "    X_tilde = stack([stack(list(value)).mean(axis=0) for key, value in Omega.items()])\n",
    "\n",
    "    # X_tilde = stack(list(Omega.values()))\n",
    "\n",
    "    return X_tilde, cubes\n",
    "\n",
    "def compute_robust_volumes(X_tildes, dcube_collections):\n",
    "\n",
    "    N = sum([len(X_tilde) for X_tilde in X_tildes])\n",
    "    alpha = 1.0 / (10 * N) # it means we set beta = 10\n",
    "    # print(\"alpha is :{}, and (1 + alpha) is :{}\".format(alpha, 1 + alpha))\n",
    "\n",
    "    volumes, volume_all = compute_volumes(X_tildes, d=X_tildes[0].shape[1])\n",
    "    robust_volumes = np.zeros_like(volumes)\n",
    "    for i, (volume, hypercubes) in enumerate(zip(volumes, dcube_collections)):\n",
    "        rho_omega_prod = 1.0\n",
    "        for cube_index, freq_count in hypercubes.items():\n",
    "\n",
    "            # if freq_count == 1: continue # volume does not monotonically increase with omega\n",
    "            # commenting this if will result in volume monotonically increasing with omega\n",
    "            rho_omega = (1 - alpha**(freq_count + 1)) / (1 - alpha)\n",
    "\n",
    "            rho_omega_prod *= rho_omega\n",
    "\n",
    "        robust_volumes[i] = (volume * rho_omega_prod).round(3)\n",
    "    return robust_volumes\n",
    "\n",
    "\n",
    "def robust_volume(Xs, omega=0.1):\n",
    "    # M = len(Xs)\n",
    "    D = Xs.shape[1]\n",
    "    # orderings = list(permutations(range(M)))\n",
    "\n",
    "    # s_values = torch.zeros(M)\n",
    "    # monte_carlo_s_values = torch.zeros(M)\n",
    "\n",
    "    # s_value_robust = torch.zeros(M)\n",
    "    # monts_carlo_s_values_robust = torch.zeros(M)\n",
    "\n",
    "    # Monte-carlo : shuffling the ordering and taking the first K orderings\n",
    "    # random.shuffle(orderings)\n",
    "    # K = 4 # number of permutations to sample\n",
    "    # for ordering_count, ordering in enumerate(orderings):\n",
    "\n",
    "        # prefix_vol = 0\n",
    "        # prefix_robust_vol = 0\n",
    "        # for position, i in enumerate(ordering):\n",
    "\n",
    "        #     curr_indices = set(ordering[:position+1])\n",
    "\n",
    "    # curr_train_X = torch.cat(torch.tensor(Xs.values)).reshape(-1, D)\n",
    "    curr_train_X = torch.tensor(Xs.values).reshape(-1, D)\n",
    "\n",
    "    # curr_train_X = torch.tensor(Xs.values)\n",
    "\n",
    "    # curr_vol = torch.sqrt(torch.linalg.det(curr_train_X.T @ curr_train_X) + 1e-8)\n",
    "\n",
    "\n",
    "    # marginal = curr_vol - prefix_vol\n",
    "    # prefix_vol = curr_vol\n",
    "    # s_values[i] += marginal\n",
    "\n",
    "    X_tilde, cubes = compute_X_tilde_and_counts(curr_train_X, omega)\n",
    "\n",
    "    robust_vol = compute_robust_volumes([X_tilde], [cubes])[0]\n",
    "\n",
    "    return robust_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_tree(train_x, train_y, test_x, test_y):\n",
    "    parameters_dict = {\"batch_size\": 64}\n",
    "    d_matrix = xgb.DMatrix(train_x, label=np.argmax(train_y, axis=1))\n",
    "    d_test_matrix = xgb.DMatrix(test_x, label=np.argmax(test_y, axis=1))\n",
    "    if train_y.shape[1] == 2:\n",
    "        parameters_dict[\"objective\"] = \"binary:logistic\"\n",
    "        parameters_dict[\"eval_metric\"] = \"logloss\"\n",
    "    elif train_y.shape[1] > 2:\n",
    "        parameters_dict[\"objective\"] = \"multi:softprob\"\n",
    "        parameters_dict['num_class'] = train_y.shape[1]\n",
    "        parameters_dict[\"disable_default_eval_metric\"] = 1\n",
    "        parameters_dict[\"eval_metric\"] = \"mlogloss\"\n",
    "    tree_model = xgb.train(parameters_dict, d_matrix, evals=[(d_matrix, \"train\"), (d_test_matrix, \"validate\")], num_boost_round=500, early_stopping_rounds=10)\n",
    "    tree_model = XGBoostModel(tree_model)\n",
    "    return tree_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of functions for the Hellinger Distance measure.\n",
    "\n",
    "Paper of reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger_distance(src_y, tar_y, type_continuous=False):\n",
    "    total_instances_per_label_src = np.sum(src_y, axis=0)\n",
    "    total_instances_per_label_tar = np.sum(tar_y, axis=0)\n",
    "    p = np.divide(total_instances_per_label_src, np.sum(total_instances_per_label_src))\n",
    "    q = np.divide(total_instances_per_label_tar, np.sum(total_instances_per_label_tar))\n",
    "    \n",
    "    return (1/math.sqrt(2)) * np.sqrt(np.sum(np.square(np.sqrt(p) - np.sqrt(q))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of functions for the performance degradation measure. Here, we train a model over one client (src) and then evaluate against another client (tar). We then compare this evaluation with a model trained and evaluated in _tar_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import OptunaConnection\n",
    "from experiment_parameters.model_builder.ModelBuilder import Director, get_training_configuration\n",
    "from experiment_parameters.model_builder.Model import XGBoostModel, KerasModel\n",
    "from sklearn.metrics import log_loss, mean_absolute_error\n",
    "import gc\n",
    "\n",
    "director = Director()\n",
    "\n",
    "def get_parameters(trial, model_type):\n",
    "    parameters = get_training_configuration(trial=trial, model_type=model_type)\n",
    "    return parameters\n",
    "\n",
    "def get_mlp(input_dim, num_classes, parameters):\n",
    "    return director.create_mlp(input_parameters=input_dim, num_classes=num_classes, parameters=parameters)\n",
    "\n",
    "def performance_degradation(train_src_x, train_src_y, train_tar_x, train_tar_y, test_tar_x, test_tar_y, dataset_name):\n",
    "    if dataset_name == \"har\":\n",
    "        study = OptunaConnection.load_study(\"mlp_har\")\n",
    "    elif dataset_name == \"edge-iot-coreset\":\n",
    "        study = OptunaConnection.load_study(\"mlp_edge_iiot_coreset\")\n",
    "    elif dataset_name == \"electric-consumption\":\n",
    "        study = OptunaConnection.load_study(\"mlp_electric_consumption\")\n",
    "    best_trial = study.best_trial\n",
    "    parameters_dict = get_training_configuration(best_trial, \"mlp\")\n",
    "\n",
    "    model_src = get_mlp(train_src_x.shape[1], train_src_y.shape[1], parameters_dict)\n",
    "    model_src.fit(train_src_x, train_src_y, epochs=30)\n",
    "\n",
    "    model_tar = get_mlp(train_src_x.shape[1], train_src_y.shape[1], parameters_dict)\n",
    "    model_tar.fit(train_tar_x, train_tar_y, epochs=30)\n",
    "\n",
    "    if train_src_y.shape[1] == 1:\n",
    "        metric_list = [\"MAE\"]\n",
    "        evaluation_result_src = evaluator(test_tar_x, test_tar_y, model_src, metric_list=metric_list).get_value_of_metric(\"MAE\")\n",
    "        evaluation_result_tar = evaluator(test_tar_x, test_tar_y, model_tar, metric_list=metric_list).get_value_of_metric(\"MAE\")\n",
    "    else:\n",
    "        metric_list = [\"CrossEntropyLoss\"]\n",
    "        evaluation_result_src = evaluator(test_tar_x, test_tar_y, model_src, metric_list=metric_list).get_value_of_metric(\"CrossEntropyLoss\")\n",
    "        evaluation_result_tar = evaluator(test_tar_x, test_tar_y, model_tar, metric_list=metric_list).get_value_of_metric(\"CrossEntropyLoss\")\n",
    "\n",
    "    return evaluation_result_src - evaluation_result_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function then computes all measures for the experiment. Each experiment represents a federated training with an existing distribution among the clients. This function then computes the measures among those datasets and the global dataset, which is the concatenation of all the datasets of the participants. The functions were listed above.\n",
    "\n",
    "__NOTE__: For the volume function, we use the [Prince](https://maxhalford.github.io/prince/) library, as it requires a full rank matrix as a input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_distances_and_volumes(dataset_name, path_to_train_datasets):\n",
    "    wassersteinDataframe = pd.DataFrame()\n",
    "    gaussianMMDDataframe = pd.DataFrame()\n",
    "    relevanceDataframe = pd.DataFrame()\n",
    "    diversityDataframe = pd.DataFrame()\n",
    "    volumeDataframe = pd.Series()\n",
    "\n",
    "    performanceDegradation = pd.DataFrame()\n",
    "    negativeConditionalEntropyDataframe = pd.DataFrame()\n",
    "    yShiftDataframe = pd.DataFrame()\n",
    "    hellingerDistanceDataframe = pd.DataFrame()\n",
    "    \n",
    "    # regularizedHScoreDataframe = pd.DataFrame()\n",
    "    classification = False\n",
    "\n",
    "    global_X_training, global_y_training = dataset_model_dictionary[dataset_name]().get_dataset().get_training_data()\n",
    "    global_X_test, global_y_test = dataset_model_dictionary[dataset_name]().get_dataset().get_test_data()\n",
    "    column_names = global_X_training.columns.values.tolist()\n",
    "\n",
    "    if len(global_y_training.shape) > 1: # Classification problem\n",
    "        classification = True\n",
    "\n",
    "    global_X_training = downcast_types(global_X_training)\n",
    "    global_X_test = downcast_types(global_X_test)\n",
    "\n",
    "    num_clients, all_combinations = number_of_clients_and_all_combinations(path_to_train_datasets)\n",
    "    volumeDataframe.loc[\"Global\"] = compute_volumes(prince.PCA(n_components=10).fit_transform(pd.concat([global_X_training, global_X_test]).reset_index(drop=True)).to_numpy())\n",
    "    volumeDataframe.loc[\"Global\"] = robust_volume(prince.PCA(n_components=10).fit_transform(pd.concat([global_X_training, global_X_test])))\n",
    "\n",
    "    for source_client in range(num_clients):\n",
    "        src_train_x = pd.read_csv(path_to_train_datasets + os.sep + \"client_\" + str(source_client) + \"_X_training.csv\", index_col=0)\n",
    "        src_train_y = pd.read_csv(path_to_train_datasets + os.sep + \"client_\" + str(source_client) + \"_y_training.csv\", index_col=0)\n",
    "        src_test_x = pd.read_csv(path_to_train_datasets + os.sep + \"client_\" + str(source_client) + \"_X_test.csv\", index_col=0)\n",
    "        src_test_y = pd.read_csv(path_to_train_datasets + os.sep + \"client_\" + str(source_client) + \"_y_test.csv\", index_col=0)\n",
    "\n",
    "        src_train_x = downcast_types(src_train_x)\n",
    "        src_test_x = downcast_types(src_test_x)\n",
    "\n",
    "        tree_model = get_xgb_tree(src_train_x, src_train_y, src_test_x, src_test_y)\n",
    "        wassersteinDataframe.loc[source_client, \"Global\"]= wasserstein_distance(pd.concat([src_train_x, src_test_x]),\n",
    "                                                                                pd.concat([global_X_training, global_X_test]))\n",
    "        gaussianMMDDataframe.loc[source_client, \"Global\"]= gaussian_mmd_distance(pd.concat([src_train_x, src_test_x]),\n",
    "                                                                                pd.concat([global_X_training, global_X_test]))\n",
    "        relevanceDataframe.loc[source_client, \"Global\"], diversityDataframe.loc[source_client, \"Global\"] = task_agnostic_data_valuation(\n",
    "            prince.PCA(n_components=10).fit_transform(pd.concat([src_train_x, src_test_x])),\n",
    "            prince.PCA(n_components=10).fit_transform(pd.concat([global_X_training, global_X_test]))\n",
    "        )\n",
    "        volumeDataframe.loc[source_client] = robust_volume(prince.PCA(n_components=10).fit_transform(pd.concat([src_train_x, src_test_x])))\n",
    "\n",
    "        if classification:\n",
    "            _, yShiftDataframe.loc[source_client, \"Global\"] = y_shift(pd.concat([src_train_x, src_test_x]).to_numpy(),\n",
    "                                                                    pd.concat([src_train_y, src_test_y]).to_numpy(),\n",
    "                                                                    pd.concat([global_X_training, global_X_test]).to_numpy(),\n",
    "                                                                    pd.concat([global_y_training, global_y_test]).to_numpy(),\n",
    "                                                                    tree_model,\n",
    "                                                                    column_names)\n",
    "            negativeConditionalEntropyDataframe.loc[source_client, \"Global\"] = negative_conditional_entropy(np.argmax(pd.concat([src_train_y, src_test_y]).astype(int).to_numpy(), axis=1), np.argmax(pd.concat([global_y_training, global_y_test]).astype(int).to_numpy(), axis=1))\n",
    "\n",
    "        performanceDegradation.loc[source_client, \"Global\"] = performance_degradation(src_train_x, src_train_y, global_X_training, global_y_training, global_X_test, global_y_test, dataset_name=dataset_name)\n",
    "        if dataset_name != \"electric-consumption\":\n",
    "            hellingerDistanceDataframe.loc[source_client, \"Global\"] = hellinger_distance(pd.concat([src_train_y, src_test_y]).to_numpy(), pd.concat([global_y_training, global_y_test]).to_numpy())\n",
    "\n",
    "        for target_client in range(num_clients):\n",
    "            tar_train_x = pd.read_csv(path_to_train_datasets + os.sep + \"client_\" + str(target_client) + \"_X_training.csv\", index_col=0)\n",
    "            tar_train_y = pd.read_csv(path_to_train_datasets + os.sep + \"client_\" + str(target_client) + \"_y_training.csv\", index_col=0)\n",
    "            tar_test_x = pd.read_csv(path_to_train_datasets + os.sep + \"client_\" + str(target_client) + \"_X_test.csv\", index_col=0)\n",
    "            tar_test_y = pd.read_csv(path_to_train_datasets + os.sep + \"client_\" + str(target_client) + \"_y_test.csv\", index_col=0)\n",
    "\n",
    "            tar_train_x = downcast_types(tar_train_x)\n",
    "            tar_test_x = downcast_types(tar_test_x)\n",
    "\n",
    "            performanceDegradation.loc[source_client, target_client] = performance_degradation(src_train_x, src_train_y, tar_train_x, tar_train_y, tar_test_x, tar_test_y, dataset_name=dataset_name)\n",
    "\n",
    "            wassersteinDataframe.loc[source_client, target_client]= wasserstein_distance(pd.concat([src_train_x, src_test_x]),\n",
    "                                                                                pd.concat([tar_train_x, tar_test_x]))\n",
    "            gaussianMMDDataframe.loc[source_client, target_client]= gaussian_mmd_distance(pd.concat([src_train_x, src_test_x]),\n",
    "                                                                                pd.concat([tar_train_x, tar_test_x]))\n",
    "            relevanceDataframe.loc[source_client, target_client], diversityDataframe.loc[source_client, target_client] = task_agnostic_data_valuation(\n",
    "                prince.PCA(n_components=10).fit_transform(pd.concat([src_train_x, src_test_x])),\n",
    "                prince.PCA(n_components=10).fit_transform(pd.concat([tar_train_x, tar_test_x]))\n",
    "            )\n",
    "\n",
    "            if classification:\n",
    "                _, yShiftDataframe.loc[source_client, target_client] = y_shift(pd.concat([src_train_x, src_test_x]).to_numpy(),\n",
    "                                                                            pd.concat([src_train_y, src_test_y]).to_numpy(),\n",
    "                                                                            pd.concat([tar_train_x, tar_test_x]).to_numpy(),\n",
    "                                                                            pd.concat([tar_train_y, tar_test_y]).to_numpy(),\n",
    "                                                                            tree_model,\n",
    "                                                                            column_names)\n",
    "                negativeConditionalEntropyDataframe.loc[source_client, target_client] = negative_conditional_entropy(np.argmax(pd.concat([src_train_y, src_test_y]).astype(int).to_numpy(), axis=1), np.argmax(pd.concat([tar_train_y, tar_test_y]).astype(int).to_numpy(), axis=1))\n",
    "            if dataset_name != \"electric-consumption\":\n",
    "                hellingerDistanceDataframe.loc[source_client, target_client] = hellinger_distance(pd.concat([src_train_y, src_test_y]).to_numpy(), pd.concat([tar_train_y, tar_test_y]).to_numpy())\n",
    "\n",
    "\n",
    "    return wassersteinDataframe, gaussianMMDDataframe, performanceDegradation, yShiftDataframe, negativeConditionalEntropyDataframe, relevanceDataframe, diversityDataframe, volumeDataframe, hellingerDistanceDataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function goes then over every single experiment, and prints all the results from the measures. They can be found in the folder /results/distances_values/{dataset_name}/{type_of_partition}/{name_of_experiment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_and_values(dataset_name, type_of_partition, additional_parameter):\n",
    "    path_to_result_dataframes = get_distances_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "    path_to_train_datasets = get_data_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "    wassersteinDataframe, gaussianMMDDataframe, performanceDegradation, yShiftDataframe, negativeConditionalEntropy, relevance, diversity, volume, hellinger = compute_all_distances_and_volumes(dataset_name, path_to_train_datasets)\n",
    "    \n",
    "    os.makedirs(path_to_result_dataframes, exist_ok=True)\n",
    "\n",
    "    if yShiftDataframe.shape[0] > 0:\n",
    "        yShiftDataframe.to_csv(path_to_result_dataframes + os.sep + \"yShiftDataframe.csv\")\n",
    "    \n",
    "    if negativeConditionalEntropy.shape[0] > 0:\n",
    "        negativeConditionalEntropy.to_csv(path_to_result_dataframes + os.sep + \"negativeConditionalEntropy.csv\")\n",
    "\n",
    "    if performanceDegradation.shape[0] > 0:\n",
    "        performanceDegradation.to_csv(path_to_result_dataframes + os.sep + \"performanceDegradation.csv\")\n",
    "\n",
    "    if hellinger.shape[0] > 0:\n",
    "        hellinger.to_csv(path_to_result_dataframes + os.sep + \"hellinger.csv\")\n",
    "\n",
    "    if wassersteinDataframe.shape[0] > 0:\n",
    "        wassersteinDataframe.to_csv(path_to_result_dataframes + os.sep + \"wasserstein.csv\")\n",
    "\n",
    "    if gaussianMMDDataframe.shape[0] > 0:\n",
    "        gaussianMMDDataframe.to_csv(path_to_result_dataframes + os.sep + \"gaussian_mmd.csv\")\n",
    "    # conditionalEntropy.to_csv(path_to_result_dataframes + os.sep + \"conditionalEntropy.csv\")\n",
    "\n",
    "    if relevance.shape[0] > 0:\n",
    "        relevance.to_csv(path_to_result_dataframes + os.sep + \"relevance.csv\")\n",
    "\n",
    "    if diversity.shape[0] > 0:\n",
    "        diversity.to_csv(path_to_result_dataframes + os.sep + \"diversity.csv\")\n",
    "    \n",
    "    if volume.shape[0] > 0:\n",
    "        volume.to_csv(path_to_result_dataframes + os.sep + \"volume.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the experiments for which we will compute the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_configurations_for_distances = [\n",
    "    (\"har\", \"dirichlet\", \"1\"),\n",
    "    (\"har\", \"dirichlet\", \"10\"),\n",
    "    (\"har\", \"dirichlet\", \"100\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_label_skew\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_LessLabels\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_MissingTwoLabels\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Laying\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_1\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_2\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_3\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_4\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_5\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_2\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_3\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_4\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_5\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"10\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"100\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"1000\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"10000\"),\n",
    "    (\"edge-iot-coreset\", \"manual\", \"EdgeIIOT_Maverick_LeastAttack\"),\n",
    "    (\"edge-iot-coreset\", \"manual\", \"EdgeIIOT_Maverick_OnlyNormal\"),\n",
    "    (\"electric-consumption\", \"manual\", \"Electric_Consumption_Random_Sampling\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionFacilityType\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionNoniid\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionStateFactor\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumption_FeatureSkew_MaverickMultifamilyUncategorized\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumption_FeatureSkew_MaverickGroceryStore\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, type_of_partition, additional_parameter in training_configurations_for_distances:\n",
    "    compute_distances_and_values(dataset_name, type_of_partition, additional_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function then reads all the measure results and prints them in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_of_training(dataset_name, type_of_partition, additional_parameter):\n",
    "    results_dataframe = pd.Series()\n",
    "\n",
    "    path_to_result_dataframes = get_results_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "\n",
    "    # metrics = []\n",
    "    # result = []\n",
    "\n",
    "    results_dataframe.loc[\"Dataset\"] = dataset_name\n",
    "    results_dataframe.loc[\"Type_of_partition\"] = type_of_partition\n",
    "    results_dataframe.loc[\"dirichlet/NamePartition\"] = additional_parameter\n",
    "\n",
    "    for file in os.listdir(path_to_result_dataframes + os.sep + \"Evaluation\"):\n",
    "        # spearman_rank_distance_sv = pd.DataFrame(columns=[\"Dataset\", \"TypePartition\", \"AdditionalParameter\", \"Metric\", \"Evaluator\", \"Distance\", \"SpearmanRank\", \"p-value\"])\n",
    "        metric_name = file.split(\"_\")[1]\n",
    "\n",
    "        if file != \"Evaluation_F1Score\":\n",
    "            evaluation_dataframe = pd.read_csv(path_to_result_dataframes + os.sep + \"Evaluation\" + os.sep + file)\n",
    "            results_dataframe.loc[metric_name] = evaluation_dataframe.loc[evaluation_dataframe.index[-1], \"Global\"]\n",
    "\n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataframe = pd.DataFrame()\n",
    "\n",
    "for dataset_name, type_of_partition, additional_parameter in training_configurations_for_distances:\n",
    "    # display(get_results_of_training(dataset_name, type_of_partition, additional_parameter))\n",
    "    results_dataframe = pd.concat([results_dataframe, get_results_of_training(dataset_name, type_of_partition, additional_parameter).to_frame().T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Correlations between values and distances (2nd Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing distances and value for the different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_configurations = [\n",
    "    (\"har\", \"dirichlet\", \"1\"),\n",
    "    (\"har\", \"dirichlet\", \"10\"),\n",
    "    (\"har\", \"dirichlet\", \"100\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_label_skew\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_LessLabels\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_MissingTwoLabels\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Laying\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_1\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_2\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_3\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_4\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_5\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_2\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_3\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_4\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_5\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"10\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"100\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"1000\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"10000\"),\n",
    "    (\"edge-iot-coreset\", \"manual\", \"EdgeIIOT_Maverick_LeastAttack\"),\n",
    "    (\"edge-iot-coreset\", \"manual\", \"EdgeIIOT_Maverick_OnlyNormal\"),\n",
    "    (\"electric-consumption\", \"manual\", \"Electric_Consumption_Random_Sampling\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionFacilityType\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionNoniid\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionStateFactor\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumption_FeatureSkew_MaverickMultifamilyUncategorized\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumption_FeatureSkew_MaverickGroceryStore\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions compute the spearman and pearson correlation for the sv. In this case, we are comparing the sv computed both in an centralized and distributed setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearman_correlation_sv(dataset_name, type_of_partition, additional_parameter):\n",
    "    path_to_result_dataframes = get_results_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "\n",
    "    spearman_rank_sv_metric_centralized_decentralized = pd.DataFrame(columns=[\"Dataset\", \"TypePartition\", \"AdditionalParameter\", \"Metric\", \"SpearmanRank\", \"p-value\"])\n",
    "    spearman_sv_metrics = []\n",
    "    spearman_sv_values = []\n",
    "    spearman_sv_p_values = []\n",
    "\n",
    "    for file in os.listdir(path_to_result_dataframes + os.sep + \"Shapley_Value\"):\n",
    "        results_dataframe = pd.read_csv(path_to_result_dataframes + os.sep + \"Shapley_Value\" + os.sep + file)\n",
    "        # display(path_to_result_dataframes)\n",
    "        # display(file)\n",
    "        spearman_sv_metrics.append(file.split('_')[1])\n",
    "        if \"SV_F1Score\" == file:\n",
    "            grouped_sv = results_dataframe.groupby([\"Evaluator\", \"Classes\"]).sum().reindex(sorted(results_dataframe.columns), axis=1).drop([\"Evaluator\", \"Classes\", \"Round\"], axis=1)\n",
    "            spearman_sv_values.append(0)\n",
    "            spearman_sv_p_values.append(0)\n",
    "        else:\n",
    "            grouped_sv = results_dataframe.groupby([\"Evaluator\"]).sum().reindex(sorted(results_dataframe.columns), axis=1).drop([\"Evaluator\", \"Round\"], axis=1)\n",
    "            # display(grouped_sv)\n",
    "        # spearman_rank, pvalue = scipy.stats.spearmanr(grouped_sv.loc[\"Centralized\"], grouped_sv.loc[\"Aggregated\"])\n",
    "            def statistic(x): # permute only `x`\n",
    "                return scipy.stats.spearmanr(x, grouped_sv.loc[\"Aggregated\"]).statistic\n",
    "            res_exact = scipy.stats.permutation_test((grouped_sv.loc[\"Centralized\"],), statistic,\n",
    "                permutation_type='pairings')\n",
    "            # res_asymptotic = scipy.stats.spearmanr(grouped_sv.loc[\"Centralized\"], grouped_sv.loc[\"Aggregated\"])\n",
    "            # res_exact.pvalue # asymptotic pvalue is too low\n",
    "            # display(res_exact.statistic)\n",
    "            # display(res_exact.pvalue)\n",
    "            spearman_sv_values.append(res_exact.statistic)\n",
    "            spearman_sv_p_values.append(res_exact.pvalue)\n",
    "            # display(spearman_sv_values)\n",
    "            # display(spearman_sv_p_values)\n",
    "\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"Dataset\"] = [dataset_name for _ in range(len(spearman_sv_metrics))]\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"TypePartition\"] = [type_of_partition for _ in range(len(spearman_sv_metrics))]\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"AdditionalParameter\"] = [additional_parameter for _ in range(len(spearman_sv_metrics))]\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"Metric\"] = spearman_sv_metrics\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"SpearmanRank\"] = spearman_sv_values\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"p-value\"] = spearman_sv_p_values\n",
    "\n",
    "    return spearman_rank_sv_metric_centralized_decentralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson_correlation_sv(dataset_name, type_of_partition, additional_parameter):\n",
    "    path_to_result_dataframes = get_results_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "\n",
    "    pearson_rank_sv_metric_centralized_decentralized = pd.DataFrame(columns=[\"Dataset\", \"TypePartition\", \"AdditionalParameter\", \"Metric\", \"PearsonCorrelation\", \"p-value\"])\n",
    "    pearson_sv_metrics = []\n",
    "    pearson_sv_values = []\n",
    "    pearson_sv_p_values = []\n",
    "\n",
    "    for file in os.listdir(path_to_result_dataframes + os.sep + \"Shapley_Value\"):\n",
    "        results_dataframe = pd.read_csv(path_to_result_dataframes + os.sep + \"Shapley_Value\" + os.sep + file)\n",
    "        # display(path_to_result_dataframes)\n",
    "        # display(file)\n",
    "        pearson_sv_metrics.append(file.split('_')[1])\n",
    "        if file == \"SV_F1Score\":\n",
    "            grouped_sv = results_dataframe.groupby([\"Evaluator\", \"Classes\"]).sum().reindex(sorted(results_dataframe.columns), axis=1).drop([\"Evaluator\", \"Classes\", \"Round\"], axis=1)\n",
    "            pearson_sv_values.append(0)\n",
    "            pearson_sv_p_values.append(0)\n",
    "        elif file == \"SV_CosineSimilarity\":\n",
    "            grouped_sv = results_dataframe.groupby([\"Evaluator\"]).sum().reindex(sorted(results_dataframe.columns), axis=1).drop([\"Evaluator\", \"Round\"], axis=1)\n",
    "            pearson_sv_values.append(0)\n",
    "            pearson_sv_p_values.append(0)\n",
    "        else:\n",
    "            grouped_sv = results_dataframe.groupby([\"Evaluator\"]).sum().reindex(sorted(results_dataframe.columns), axis=1).drop([\"Evaluator\", \"Round\"], axis=1)\n",
    "            # def statistic(x): # permute only `x`\n",
    "            #     return scipy.stats.pearsonr(x, grouped_sv.loc[\"Aggregated\"]).statistic\n",
    "            # res_exact = scipy.stats.permutation_test((grouped_sv.loc[\"Centralized\"],), statistic,\n",
    "            #     permutation_type='pairings')\n",
    "            res_exact = scipy.stats.pearsonr(grouped_sv.loc[\"Centralized\"], grouped_sv.loc[\"Aggregated\"])\n",
    "            pearson_sv_values.append(res_exact.statistic)\n",
    "            pearson_sv_p_values.append(res_exact.pvalue)\n",
    "\n",
    "    pearson_rank_sv_metric_centralized_decentralized[\"Dataset\"] = [dataset_name for _ in range(len(pearson_sv_metrics))]\n",
    "    pearson_rank_sv_metric_centralized_decentralized[\"TypePartition\"] = [type_of_partition for _ in range(len(pearson_sv_metrics))]\n",
    "    pearson_rank_sv_metric_centralized_decentralized[\"AdditionalParameter\"] = [additional_parameter for _ in range(len(pearson_sv_metrics))]\n",
    "    pearson_rank_sv_metric_centralized_decentralized[\"Metric\"] = pearson_sv_metrics\n",
    "    pearson_rank_sv_metric_centralized_decentralized[\"PearsonCorrelation\"] = pearson_sv_values\n",
    "    pearson_rank_sv_metric_centralized_decentralized[\"p-value\"] = pearson_sv_p_values\n",
    "\n",
    "    return pearson_rank_sv_metric_centralized_decentralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_rank_sv_metric_centralized_decentralized = pd.DataFrame(columns=[\"Dataset\", \"TypePartition\", \"AdditionalParameter\", \"Metric\", \"SpearmanRank\", \"p-value\"])\n",
    "pearson_correlation_sv_metric_centralized_decentralized = pd.DataFrame(columns=[\"Dataset\", \"TypePartition\", \"AdditionalParameter\", \"Metric\", \"PearsonCorrelation\", \"p-value\"])\n",
    "\n",
    "\n",
    "for dataset_name, type_of_partition, additional_parameter in training_configurations:\n",
    "    spearman_rank_sv_metric_centralized_decentralized = pd.concat([spearman_rank_sv_metric_centralized_decentralized, compute_spearman_correlation_sv(dataset_name, type_of_partition, additional_parameter)], ignore_index=True)\n",
    "    pearson_correlation_sv_metric_centralized_decentralized = pd.concat([pearson_correlation_sv_metric_centralized_decentralized, compute_pearson_correlation_sv(dataset_name, type_of_partition, additional_parameter)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman_rank_sv_metric_centralized_decentralized_no_f1 = spearman_rank_sv_metric_centralized_decentralized[(spearman_rank_sv_metric_centralized_decentralized[\"Metric\"] != \"F1Score\") & (spearman_rank_sv_metric_centralized_decentralized[\"Metric\"] != \"CosineSimilarity\")]\n",
    "\n",
    "# spearman_rank_sv_metric_centralized_decentralized_no_f1[\"SpearmanRank\"] = spearman_rank_sv_metric_centralized_decentralized_no_f1[\"SpearmanRank\"].astype('float')\n",
    "# spearman_rank_sv_metric_centralized_decentralized_no_f1[\"p-value\"] = spearman_rank_sv_metric_centralized_decentralized_no_f1[\"p-value\"].astype('float')\n",
    "\n",
    "# pearson_correlation_sv_metric_centralized_decentralized_no_f1 = pearson_correlation_sv_metric_centralized_decentralized[(pearson_correlation_sv_metric_centralized_decentralized[\"Metric\"] != \"F1Score\") & (spearman_rank_sv_metric_centralized_decentralized[\"Metric\"] != \"CosineSimilarity\")]\n",
    "\n",
    "# pearson_correlation_sv_metric_centralized_decentralized_no_f1[\"PearsonCorrelation\"] = pearson_correlation_sv_metric_centralized_decentralized_no_f1[\"PearsonCorrelation\"].astype('float')\n",
    "# pearson_correlation_sv_metric_centralized_decentralized_no_f1[\"p-value\"] = pearson_correlation_sv_metric_centralized_decentralized_no_f1[\"p-value\"].astype('float')\n",
    "\n",
    "# display(spearman_rank_sv_metric_centralized_decentralized_no_f1.groupby([\"Dataset\"]).mean(numeric_only=True))\n",
    "\n",
    "# display(spearman_rank_sv_metric_centralized_decentralized_no_f1.groupby([\"Dataset\", \"Metric\"]).mean(numeric_only=True))\n",
    "\n",
    "# display(spearman_rank_sv_metric_centralized_decentralized_no_f1.groupby([\"Metric\"]).mean(numeric_only=True))\n",
    "\n",
    "# display(spearman_rank_sv_metric_centralized_decentralized_no_f1[spearman_rank_sv_metric_centralized_decentralized_no_f1[\"SpearmanRank\"] != 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(spearman_rank_sv_metric_centralized_decentralized_no_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pearson_correlation_sv_metric_centralized_decentralized_no_f1.groupby([\"Dataset\"]).mean(numeric_only=True))\n",
    "\n",
    "# display(pearson_correlation_sv_metric_centralized_decentralized_no_f1.groupby([\"Dataset\", \"Metric\"]).mean(numeric_only=True))\n",
    "\n",
    "# display(pearson_correlation_sv_metric_centralized_decentralized_no_f1[pearson_correlation_sv_metric_centralized_decentralized_no_f1[\"PearsonCorrelation\"] != 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_results_dataframes = get_results_from_route(\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_1\")\n",
    "\n",
    "results_dataframe = pd.read_csv(path_to_results_dataframes + os.sep + \"Shapley_Value\" + os.sep + \"SV_CrossEntropyLoss\")\n",
    "grouped_sv = results_dataframe.groupby([\"Evaluator\"]).sum().reindex(sorted(results_dataframe.columns), axis=1).drop([\"Evaluator\", \"Round\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these two functions compute the spearman and pearson correlation coefficient comparing the distances computed with the centralized SV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearman_correlation_distance(dataset_name, type_of_partition, additional_parameter, type_of_loss):\n",
    "    path_to_distance_dataframes = get_distances_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "    path_to_results_dataframes = get_results_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "\n",
    "    spearman_rank_sv_metric_centralized_decentralized = pd.DataFrame()\n",
    "    distances = []\n",
    "    spearman_values = []\n",
    "    spearman_p_values = []\n",
    "\n",
    "    print(f\"Dataset name: {dataset_name}\")\n",
    "    print(f\"Type_of_partition name: {type_of_partition}\")\n",
    "    print(f\"Additional_parameter name: {additional_parameter}\")\n",
    "    \n",
    "    if type_of_loss == \"MAE\":\n",
    "        results_dataframe = pd.read_csv(path_to_results_dataframes + os.sep + \"Shapley_Value\" + os.sep + \"SV_MAE\")\n",
    "    else:\n",
    "        results_dataframe = pd.read_csv(path_to_results_dataframes + os.sep + \"Shapley_Value\" + os.sep + \"SV_CrossEntropyLoss\")\n",
    "    \n",
    "    grouped_sv = results_dataframe.groupby([\"Evaluator\"]).sum().reindex(sorted(results_dataframe.columns), axis=1).drop([\"Evaluator\", \"Round\"], axis=1)\n",
    "    grouped_sv.drop(\"Aggregated\", inplace=True)\n",
    "\n",
    "    # display(grouped_sv)\n",
    "\n",
    "    for file in os.listdir(path_to_distance_dataframes):\n",
    "        distances.append(file.split('.')[0])\n",
    "        if file == \"volume.csv\":\n",
    "            volume_series = pd.read_csv(path_to_distance_dataframes + os.sep + file, index_col=0)\n",
    "            volume_series.drop(\"Global\", inplace=True)\n",
    "            def statistic(x): # permute only `x`\n",
    "                return scipy.stats.spearmanr(x, grouped_sv.loc[\"Centralized\"]).statistic\n",
    "            res_exact = scipy.stats.permutation_test((volume_series.loc[:],), statistic,\n",
    "                permutation_type='pairings')\n",
    "            spearman_values.append(res_exact.statistic[0])\n",
    "            spearman_p_values.append(res_exact.pvalue[0])\n",
    "\n",
    "        else:\n",
    "            distance_dataframe = pd.read_csv(path_to_distance_dataframes + os.sep + file, index_col=0)\n",
    "            spearman_list = []\n",
    "            pvalue_list = []\n",
    "            for index, row in grouped_sv.iterrows():\n",
    "                if index == \"Centralized\":\n",
    "                    # def statistic(x): # permute only `x`\n",
    "                    #     return scipy.stats.spearmanr(x, grouped_sv.loc[\"Centralized\"]).statistic\n",
    "                    # res_exact = scipy.stats.permutation_test((distance_dataframe.loc[:, \"Global\"],), statistic,\n",
    "                    #     permutation_type='pairings')\n",
    "                    res_exact = scipy.stats.spearmanr(grouped_sv.loc[\"Centralized\"], distance_dataframe.loc[:, \"Global\"])\n",
    "                    spearman_list.append(res_exact.statistic)\n",
    "                    pvalue_list.append(res_exact.pvalue)\n",
    "                else:\n",
    "                    # def statistic(x): # permute only `x`\n",
    "                    #     return scipy.stats.spearmanr(x, grouped_sv.loc[str(index)]).statistic\n",
    "                    # res_exact = scipy.stats.permutation_test((distance_dataframe.loc[:, str(index)],), statistic,\n",
    "                    #     permutation_type='pairings')\n",
    "                    res_exact = scipy.stats.spearmanr(grouped_sv.loc[str(index)], distance_dataframe.loc[:, str(index)])\n",
    "                    spearman_list.append(res_exact.statistic)\n",
    "                    pvalue_list.append(res_exact.pvalue)\n",
    "\n",
    "            # Fisher transformation for averaging correlations.\n",
    "            spearman_values.append(np.tanh(np.mean(np.arctanh(spearman_list))))\n",
    "            spearman_p_values.append(np.mean(pvalue_list))\n",
    "\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"Dataset\"] = [dataset_name for _ in range(len(distances))]\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"TypePartition\"] = [type_of_partition for _ in range(len(distances))]\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"AdditionalParameter\"] = [additional_parameter for _ in range(len(distances))]\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"Distance\"] = distances\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"SpearmanCorrelation\"] = spearman_values\n",
    "    spearman_rank_sv_metric_centralized_decentralized[\"p-value-spearman\"] = spearman_p_values\n",
    "\n",
    "    return spearman_rank_sv_metric_centralized_decentralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson_correlation_distance(dataset_name, type_of_partition, additional_parameter, type_of_loss=\"CrossEntropyLoss\"):\n",
    "    path_to_distance_dataframes = get_distances_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "    path_to_results_dataframes = get_results_from_route(dataset_name, type_of_partition, additional_parameter)\n",
    "\n",
    "    pearson_sv_metric_centralized_decentralized = pd.DataFrame()\n",
    "    distances = []\n",
    "    pearson_values = []\n",
    "    pearson_p_values = []\n",
    "    variance_scenario = []\n",
    "\n",
    "    print(f\"Dataset name: {dataset_name}\")\n",
    "    print(f\"Type_of_partition name: {type_of_partition}\")\n",
    "    print(f\"Additional_parameter name: {additional_parameter}\")\n",
    "\n",
    "    if type_of_loss == \"MAE\":\n",
    "        results_dataframe = pd.read_csv(path_to_results_dataframes + os.sep + \"Shapley_Value\" + os.sep + \"SV_MAE\")\n",
    "    else:\n",
    "        results_dataframe = pd.read_csv(path_to_results_dataframes + os.sep + \"Shapley_Value\" + os.sep + \"SV_CrossEntropyLoss\")\n",
    "        \n",
    "    grouped_sv = results_dataframe.groupby([\"Evaluator\"]).sum().reindex(sorted(results_dataframe.columns), axis=1).drop([\"Evaluator\", \"Round\"], axis=1)\n",
    "    grouped_sv.drop(\"Aggregated\", inplace=True)\n",
    "\n",
    "    # display(grouped_sv)\n",
    "\n",
    "    for file in os.listdir(path_to_distance_dataframes):\n",
    "        distances.append(file.split('.')[0])\n",
    "        if file == \"volume.csv\":\n",
    "            volume_series = pd.read_csv(path_to_distance_dataframes + os.sep + file, index_col=0)\n",
    "            volume_series.drop(\"Global\", inplace=True)\n",
    "            res_exact = scipy.stats.pearsonr(grouped_sv.loc[\"Centralized\"], volume_series.to_numpy().reshape(-1))\n",
    "            pearson_values.append(res_exact.statistic)\n",
    "            pearson_p_values.append(res_exact.pvalue)\n",
    "            # variance_scenario.append(np.var(grouped_sv.loc[\"Centralized\"]))\n",
    "        else:\n",
    "            distance_dataframe = pd.read_csv(path_to_distance_dataframes + os.sep + file, index_col=0)\n",
    "            if file == \"yShiftDataframe.csv\":\n",
    "                distance_dataframe.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "                distance_dataframe.replace([np.NAN], 0, inplace=True)\n",
    "            pearsonr_list = []\n",
    "            pvalue_list = []\n",
    "            for index, row in grouped_sv.iterrows():\n",
    "                if index == \"Centralized\":\n",
    "                    res_exact = scipy.stats.pearsonr(grouped_sv.loc[\"Centralized\"], distance_dataframe.loc[:, \"Global\"])\n",
    "                    pearsonr_list.append(res_exact.statistic)\n",
    "                    pvalue_list.append(res_exact.pvalue)\n",
    "                else:            \n",
    "                    res_exact = scipy.stats.pearsonr(grouped_sv.loc[str(index)], distance_dataframe.loc[:, str(index)])\n",
    "                    pearsonr_list.append(res_exact.statistic)\n",
    "                    pvalue_list.append(res_exact.pvalue)\n",
    "\n",
    "            # variance_scenario.append(np.var(grouped_sv.loc[\"Centralized\"]))\n",
    "            # Fisher transformation for averaging correlations.\n",
    "            pearson_values.append(np.tanh(np.mean(np.arctanh(pearsonr_list))))\n",
    "            pearson_p_values.append(np.mean(pvalue_list))\n",
    "\n",
    "    pearson_sv_metric_centralized_decentralized[\"Dataset\"] = [dataset_name for _ in range(len(distances))]\n",
    "    pearson_sv_metric_centralized_decentralized[\"TypePartition\"] = [type_of_partition for _ in range(len(distances))]\n",
    "    pearson_sv_metric_centralized_decentralized[\"AdditionalParameter\"] = [additional_parameter for _ in range(len(distances))]\n",
    "    pearson_sv_metric_centralized_decentralized[\"Distance\"] = distances\n",
    "    pearson_sv_metric_centralized_decentralized[\"PearsonCorrelation\"] = pearson_values\n",
    "    pearson_sv_metric_centralized_decentralized[\"p-value-pearson\"] = pearson_p_values\n",
    "    # pearson_sv_metric_centralized_decentralized[\"VarianceSV\"] = variance_scenario\n",
    "\n",
    "    return pearson_sv_metric_centralized_decentralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_to_compare = [\n",
    "    (\"har\", \"dirichlet\", \"1\"),\n",
    "    (\"har\", \"dirichlet\", \"10\"),\n",
    "    (\"har\", \"dirichlet\", \"100\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_label_skew\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_LessLabels\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_MissingTwoLabels\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Laying\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_1\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_2\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_3\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_4\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_5\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_2\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_3\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_4\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_5\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"10\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"100\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"1000\"),\n",
    "    (\"edge-iot-coreset\", \"dirichlet\", \"10000\"),\n",
    "    (\"edge-iot-coreset\", \"manual\", \"EdgeIIOT_Maverick_LeastAttack\"),\n",
    "    (\"edge-iot-coreset\", \"manual\", \"EdgeIIOT_Maverick_OnlyNormal\"),\n",
    "    (\"electric-consumption\", \"manual\", \"Electric_Consumption_Random_Sampling\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionFacilityType\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionNoniid\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumptionStateFactor\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumption_FeatureSkew_MaverickMultifamilyUncategorized\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumption_FeatureSkew_MaverickGroceryStore\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_rank_sv_distance = pd.DataFrame()\n",
    "pearson_rank_sv_distance = pd.DataFrame()\n",
    "\n",
    "\n",
    "for dataset_name, type_of_partition, additional_parameter, type_of_loss in distances_to_compare:\n",
    "    spearman_rank_sv_distance = pd.concat([spearman_rank_sv_distance, compute_spearman_correlation_distance(dataset_name, type_of_partition, additional_parameter, type_of_loss)], ignore_index=True)\n",
    "    pearson_rank_sv_distance = pd.concat([pearson_rank_sv_distance, compute_pearson_correlation_distance(dataset_name, type_of_partition, additional_parameter, type_of_loss)], ignore_index=True)\n",
    "\n",
    "correlation_sv_distance = spearman_rank_sv_distance.copy()\n",
    "correlation_sv_distance[\"PearsonCorrelation\"] = pearson_rank_sv_distance[\"PearsonCorrelation\"]\n",
    "correlation_sv_distance[\"p-value-pearson\"] = pearson_rank_sv_distance[\"p-value-pearson\"]\n",
    "# correlation_sv_distance[\"VarianceSV\"] = pearson_rank_sv_distance[\"VarianceSV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to create the box plot with the Pearson correlation coefficient for all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "#garbage graph\n",
    "fig = px.scatter(x=[0, 1, 2, 3, 4], y=[0, 1, 4, 9, 16])\n",
    "fig.show()\n",
    "fig.write_image(\"random.pdf\")\n",
    "\n",
    "\n",
    "distance_dictionary = {\n",
    "    \"performanceDegradation\": \"Performance Degradation (I)\",\n",
    "    \"yShiftDataframe\": \"X->Y Shift (I)\",\n",
    "    \"relevance\": \"Relevance (D)\",\n",
    "    \"diversity\": \"Diversity (D)\",\n",
    "    \"hellinger\": \"Hellinger Distance (I)\",\n",
    "    \"wasserstein\": \"Wasserstein Distance (I)\",\n",
    "    \"gaussian_mmd\": \"MMD - Gaussian Kernel (I)\",\n",
    "    \"volume\": \"Volume of data (D)\"\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "for metric, name_of_metric in distance_dictionary.items():\n",
    "    sliced_df = correlation_sv_distance[correlation_sv_distance[\"Distance\"] == metric]\n",
    "    fig.add_trace(go.Box(y=sliced_df[\"PearsonCorrelation\"], name=name_of_metric))\n",
    "\n",
    "\n",
    "fig.update_layout(title_text=\"Pearson Correlation between Distance and Value\", showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"images/distance_value_pearson_correlation.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to create the box plot with the Pearson correlation coefficient for only the cases with Mavericks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_only_mavericks_to_compare = [\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_label_skew\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_LessLabels\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_MissingTwoLabels\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Laying\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_1\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_2\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_3\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_4\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_Balanced_Laying_5\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_2\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_3\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_4\", \"CrossEntropyLoss\"),\n",
    "    (\"har\", \"manual\", \"HAR_1_Maverick_1_Balanced_WalkingUpstairs_5\", \"CrossEntropyLoss\"),\n",
    "    (\"edge-iot-coreset\", \"manual\", \"EdgeIIOT_Maverick_LeastAttack\", \"CrossEntropyLoss\"),\n",
    "    (\"edge-iot-coreset\", \"manual\", \"EdgeIIOT_Maverick_OnlyNormal\", \"CrossEntropyLoss\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumption_FeatureSkew_MaverickMultifamilyUncategorized\", \"MAE\"),\n",
    "    (\"electric-consumption\", \"manual\", \"ElectricConsumption_FeatureSkew_MaverickGroceryStore\", \"MAE\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_rank_sv_distance = pd.DataFrame()\n",
    "pearson_rank_sv_distance = pd.DataFrame()\n",
    "\n",
    "\n",
    "for dataset_name, type_of_partition, additional_parameter, type_of_loss in distances_only_mavericks_to_compare:\n",
    "    spearman_rank_sv_distance = pd.concat([spearman_rank_sv_distance, compute_spearman_correlation_distance(dataset_name, type_of_partition, additional_parameter, type_of_loss)], ignore_index=True)\n",
    "    pearson_rank_sv_distance = pd.concat([pearson_rank_sv_distance, compute_pearson_correlation_distance(dataset_name, type_of_partition, additional_parameter, type_of_loss)], ignore_index=True)\n",
    "\n",
    "correlation_sv_distance_only_mavericks = spearman_rank_sv_distance.copy()\n",
    "correlation_sv_distance_only_mavericks[\"PearsonCorrelation\"] = pearson_rank_sv_distance[\"PearsonCorrelation\"]\n",
    "correlation_sv_distance_only_mavericks[\"p-value-pearson\"] = pearson_rank_sv_distance[\"p-value-pearson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "distance_dictionary = {\n",
    "    \"performanceDegradation\": \"Performance Degradation (I)\",\n",
    "    \"yShiftDataframe\": \"X->Y Shift (I)\",\n",
    "    \"relevance\": \"Relevance (D)\",\n",
    "    \"diversity\": \"Diversity (D)\",\n",
    "    \"hellinger\": \"Hellinger Distance (I)\",\n",
    "    \"wasserstein\": \"Wasserstein Distance (I)\",\n",
    "    \"gaussian_mmd\": \"MMD - Gaussian Kernel (I)\",\n",
    "    \"volume\": \"Volume of data (D)\"\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "for metric, name_of_metric in distance_dictionary.items():\n",
    "    sliced_df = correlation_sv_distance_only_mavericks[correlation_sv_distance_only_mavericks[\"Distance\"] == metric]\n",
    "    fig.add_trace(go.Box(y=sliced_df[\"PearsonCorrelation\"], name=name_of_metric))\n",
    "\n",
    "\n",
    "fig.update_layout(title_text=\"Pearson Correlation between Distance and Value only Mavericks\", showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"images/distance_value_only_mavericks_pearson_correlation.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
